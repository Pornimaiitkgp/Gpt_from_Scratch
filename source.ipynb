{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9bac17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: [' ', 'd', 'e', 'h', 'l', 'o', 'r', 'w']\n",
      "\n",
      "Character to Integer mapping:\n",
      "{' ': 0, 'd': 1, 'e': 2, 'h': 3, 'l': 4, 'o': 5, 'r': 6, 'w': 7}\n",
      "\n",
      "Encoded sequence: [3, 2, 4, 4, 5, 0, 7, 5, 6, 4, 1]\n",
      "\n",
      "Decoded text: hello world\n"
     ]
    }
   ],
   "source": [
    "# Character level tokenisation\n",
    "\n",
    "# Step 1: Input text\n",
    "text = \"hello world\"\n",
    "\n",
    "# Step 2: Get all unique characters in the text\n",
    "vocab = sorted(list(set(text)))\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "# Step 3: Create mappings: char → int and int → char\n",
    "char_to_int = {ch: i for i, ch in enumerate(vocab)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "print(\"\\nCharacter to Integer mapping:\")\n",
    "print(char_to_int)\n",
    "\n",
    "# Step 4: Encode (text → integers)\n",
    "encoded = [char_to_int[ch] for ch in text]\n",
    "print(\"\\nEncoded sequence:\", encoded)\n",
    "\n",
    "# Step 5: Decode (integers → text)\n",
    "decoded = ''.join([int_to_char[i] for i in encoded])\n",
    "print(\"\\nDecoded text:\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe0705dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram count matrix:\n",
      " tensor([[0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 1, 1, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0]], dtype=torch.int32)\n",
      "\n",
      "Probability matrix:\n",
      " tensor([[0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.2222],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
      "        [0.1111, 0.1111, 0.1111, 0.1111, 0.2222, 0.1111, 0.1111, 0.1111],\n",
      "        [0.1111, 0.1111, 0.2222, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111],\n",
      "        [0.0909, 0.1818, 0.0909, 0.0909, 0.1818, 0.1818, 0.0909, 0.0909],\n",
      "        [0.2000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.1000],\n",
      "        [0.1111, 0.1111, 0.1111, 0.1111, 0.2222, 0.1111, 0.1111, 0.1111],\n",
      "        [0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.2222, 0.1111, 0.1111]])\n",
      "\n",
      "Generated text: elredeloo lrdwrrwd  w\n"
     ]
    }
   ],
   "source": [
    "# Bigram Model\n",
    "\n",
    "# Count bigrams (pairs of consecutive chars)\n",
    "import torch\n",
    "vocab_size=len(vocab)\n",
    "# Create a count matrix (vocab_size x vocab_size)\n",
    "N = torch.zeros((vocab_size, vocab_size), dtype=torch.int32)\n",
    "for ch1, ch2 in zip(encoded, encoded[1:]):\n",
    "    N[ch1, ch2] += 1\n",
    "print(\"Bigram count matrix:\\n\", N)\n",
    "\n",
    "\n",
    "# Convert counts → probabilities\n",
    "# Add 1 for smoothing (avoid division by zero)\n",
    "P = (N + 1).float()\n",
    "P /= P.sum(1, keepdim=True)\n",
    "print(\"\\nProbability matrix:\\n\", P)\n",
    "\n",
    "\n",
    "# Generate text using the bigram model\n",
    "import torch\n",
    "# Start with a random character\n",
    "ix = torch.randint(0, vocab_size, (1,)).item()\n",
    "out = [ix]\n",
    "for _ in range(20):  # generate 20 characters\n",
    "    # Sample next char based on probability\n",
    "    probs = P[ix]\n",
    "    ix = torch.multinomial(probs, num_samples=1).item()\n",
    "    out.append(ix)\n",
    "# Decode indices to characters\n",
    "generated_text = ''.join([int_to_char[i] for i in out])\n",
    "print(\"\\nGenerated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93b4365c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token Embeddings shape: torch.Size([11, 8])\n",
      "tensor([[ 0.1138,  2.9655,  0.0813, -0.0235, -1.3065,  1.4957, -1.0961, -0.0795],\n",
      "        [ 0.7026,  0.6871, -0.4664,  0.9837,  1.9989,  0.6514,  0.5614, -0.6657],\n",
      "        [ 1.4388, -1.5915,  0.0490, -0.3316,  0.3347, -0.6483,  0.5391,  0.8519],\n",
      "        [ 1.4388, -1.5915,  0.0490, -0.3316,  0.3347, -0.6483,  0.5391,  0.8519],\n",
      "        [ 0.2852,  0.2791,  1.7491,  0.3668, -0.7457,  0.9607,  2.3469, -0.6264],\n",
      "        [-1.0757, -0.8299, -2.3622, -0.2736, -0.1199, -1.0613, -0.6246,  0.9568],\n",
      "        [-1.3614, -0.1579,  0.5192, -0.6509, -0.2592,  0.1960,  1.0490, -1.1486],\n",
      "        [ 0.2852,  0.2791,  1.7491,  0.3668, -0.7457,  0.9607,  2.3469, -0.6264],\n",
      "        [ 1.5455,  0.5859, -0.4391,  0.6140,  1.0550,  1.5842, -0.4713, -0.7610],\n",
      "        [ 1.4388, -1.5915,  0.0490, -0.3316,  0.3347, -0.6483,  0.5391,  0.8519],\n",
      "        [-1.7192, -0.9628,  1.2277,  1.1895, -0.7530, -0.1038,  0.4013,  0.2315]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Token Embeddings\n",
    "\n",
    "import torch\n",
    "text_size = len(list(text))\n",
    "embedding_dim = 8\n",
    "# Create token embedding table\n",
    "token_embedding_table = torch.nn.Embedding(text_size, embedding_dim)\n",
    "# Convert encoded list to tensor\n",
    "tokens = torch.tensor(encoded, dtype=torch.long)\n",
    "# Lookup embeddings\n",
    "token_embeds = token_embedding_table(tokens)\n",
    "print(\"\\nToken Embeddings shape:\", token_embeds.shape)\n",
    "print(token_embeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e09fc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Position Embeddings shape: torch.Size([11, 8])\n",
      "tensor([[-0.1842,  1.3653, -0.6117, -0.3598, -1.2056,  2.5702,  0.8442,  1.3253],\n",
      "        [ 1.9362,  0.6216,  0.0669, -0.4296,  1.6063, -0.3118,  0.7864, -0.1483],\n",
      "        [-1.0733, -0.0065,  0.5474,  0.2650,  0.3834, -1.7651, -0.5644,  0.3843],\n",
      "        [ 0.4803, -0.4645,  0.1013, -0.1288,  0.1663, -1.5774, -2.2762, -0.0731],\n",
      "        [-0.4474,  0.0143, -2.0777,  0.1586, -0.3498, -0.6126, -0.4383,  0.5347],\n",
      "        [-1.3115, -0.0695, -0.4642,  1.3140, -0.6404,  1.6769,  0.3999, -1.3250],\n",
      "        [-0.5935,  0.0769, -0.5989,  2.1621,  0.2156, -0.9815,  0.2800,  0.5814],\n",
      "        [ 0.9430,  1.1100,  0.8671,  0.1920, -1.4802,  1.5407, -0.0635, -0.4202],\n",
      "        [ 0.1060,  0.1448,  1.0616, -2.7503, -0.2745,  0.9843,  1.5009, -0.1337],\n",
      "        [-0.1165, -0.6092,  0.9844,  0.1909,  0.1043, -0.0257,  2.0617, -0.7046],\n",
      "        [-1.1790,  0.0265, -0.7522, -1.0726,  0.9736,  1.1765, -1.0006, -0.0236]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Positional Embeddings\n",
    "\n",
    "# sequence_length = 11\n",
    "block_size = 11\n",
    "# Create position embedding table\n",
    "position_embedding_table = torch.nn.Embedding(block_size, embedding_dim)\n",
    "positions = torch.arange(block_size)\n",
    "position_embeds = position_embedding_table(positions)\n",
    "print(\"\\nPosition Embeddings shape:\", position_embeds.shape)\n",
    "print(position_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a7dd9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Embeddings shape: torch.Size([11, 8])\n",
      "tensor([[-0.0704,  4.3308, -0.5304, -0.3833, -2.5121,  4.0660, -0.2519,  1.2458],\n",
      "        [ 2.6387,  1.3087, -0.3995,  0.5542,  3.6052,  0.3396,  1.3477, -0.8140],\n",
      "        [ 0.3655, -1.5980,  0.5964, -0.0665,  0.7182, -2.4134, -0.0254,  1.2362],\n",
      "        [ 1.9192, -2.0560,  0.1503, -0.4603,  0.5010, -2.2257, -1.7371,  0.7788],\n",
      "        [-0.1622,  0.2934, -0.3286,  0.5253, -1.0955,  0.3482,  1.9086, -0.0916],\n",
      "        [-2.3872, -0.8995, -2.8265,  1.0404, -0.7603,  0.6156, -0.2246, -0.3682],\n",
      "        [-1.9548, -0.0810, -0.0798,  1.5111, -0.0436, -0.7855,  1.3291, -0.5672],\n",
      "        [ 1.2282,  1.3892,  2.6162,  0.5588, -2.2259,  2.5014,  2.2834, -1.0466],\n",
      "        [ 1.6515,  0.7306,  0.6225, -2.1363,  0.7805,  2.5685,  1.0296, -0.8948],\n",
      "        [ 1.3223, -2.2006,  1.0334, -0.1407,  0.4390, -0.6740,  2.6007,  0.1473],\n",
      "        [-2.8982, -0.9362,  0.4754,  0.1170,  0.2207,  1.0726, -0.5993,  0.2079]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Combine Token and Position Embeddings\n",
    "\n",
    "combined = token_embeds + position_embeds\n",
    "print(\"\\nCombined Embeddings shape:\", combined.shape)\n",
    "print(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc5e4526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape to attention: torch.Size([11, 8])\n",
      "\n",
      "Shapes -> K: torch.Size([11, 8]) | Q: torch.Size([11, 8]) | V: torch.Size([11, 8])\n",
      "\n",
      "Attention Weights shape: torch.Size([11, 11])\n",
      "\n",
      "Output shape: torch.Size([11, 8])\n",
      "\n",
      "Output (first 2 tokens):\n",
      " tensor([[ 0.4840,  0.3893, -0.4222,  0.1422,  0.4171,  0.5142, -0.1053,  0.1113],\n",
      "        [-0.6188,  0.0931,  0.3657, -0.2021, -0.4161, -0.2850,  0.1374, -0.1526]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# One Head OF Self Attention\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assume 'combined' from previous step has shape: (sequence_length, embedding_dim)\n",
    "print(\"Input shape to attention:\", combined.shape)\n",
    "\n",
    "# Step 1: Define the head size (usually embedding_dim / num_heads)\n",
    "head_size = 8  # since embedding_dim = 8\n",
    "\n",
    "# Step 2: Create linear layers for Query, Key, and Value\n",
    "key = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "query = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "value = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "\n",
    "# Step 3: Compute K, Q, V matrices\n",
    "K = key(combined)      \n",
    "Q = query(combined)    \n",
    "V = value(combined)    \n",
    "\n",
    "print(\"\\nShapes -> K:\", K.shape, \"| Q:\", Q.shape, \"| V:\", V.shape)\n",
    "\n",
    "# Step 4: Compute raw attention scores (Q × Kᵀ)\n",
    "att_scores = Q @ K.T  # shape: (seq_len, seq_len)\n",
    "att_scores = att_scores / (head_size ** 0.5)  # scale to prevent large values\n",
    "\n",
    "# Step 5: Apply softmax to get attention weights\n",
    "att_weights = F.softmax(att_scores, dim=1)\n",
    "print(\"\\nAttention Weights shape:\", att_weights.shape)\n",
    "\n",
    "# Step 6: Multiply attention weights with values (Weighted sum)\n",
    "out = att_weights @ V  # shape: (seq_len, head_size)\n",
    "\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(\"\\nOutput (first 2 tokens):\\n\", out[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6afd9795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K_heads shape: torch.Size([4, 11, 2])\n",
      "\n",
      "Multi-Head Output shape: torch.Size([11, 8])\n",
      "\n",
      "Output for first 2 tokens:\n",
      " tensor([[-0.1294, -0.2590,  0.1444,  0.0705, -0.3831, -0.1154,  0.9859,  0.1703],\n",
      "        [ 0.0533,  0.3320,  0.3313, -0.0706, -0.5334,  0.0895,  0.5451,  0.1226]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Multi Head Self Attention\n",
    "\n",
    "# Step 1: Define parameters\n",
    "embedding_dim = 8\n",
    "num_heads = 4\n",
    "head_size = embedding_dim // num_heads  # each head = 2 dimensions here\n",
    "\n",
    "# Step 2: Create Q, K, V layers for *all heads at once*\n",
    "key = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "query = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "value = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "\n",
    "# Step 3: Compute Q, K, V\n",
    "K = key(combined)     # (11, 8)\n",
    "Q = query(combined)   # (11, 8)\n",
    "V = value(combined)   # (11, 8)\n",
    "\n",
    "# Step 4: Split into heads\n",
    "# reshape to (num_heads, seq_len, head_size)\n",
    "def split_heads(x):\n",
    "    return x.view(num_heads, -1, head_size)\n",
    "\n",
    "K_heads = split_heads(K)\n",
    "Q_heads = split_heads(Q)\n",
    "V_heads = split_heads(V)\n",
    "\n",
    "print(\"\\nK_heads shape:\", K_heads.shape)  # (4, 11, 2)\n",
    "\n",
    "# Step 5: Compute attention for each head\n",
    "outputs = []\n",
    "for i in range(num_heads):\n",
    "    att_scores = Q_heads[i] @ K_heads[i].T           # (11, 11)\n",
    "    att_scores = att_scores / (head_size ** 0.5)\n",
    "    att_weights = F.softmax(att_scores, dim=1)\n",
    "    out = att_weights @ V_heads[i]                   # (11, head_size)\n",
    "    outputs.append(out)\n",
    "\n",
    "# Step 6: Concatenate all heads\n",
    "multi_head_out = torch.cat(outputs, dim=-1)  # (11, embedding_dim)\n",
    "\n",
    "# Step 7: Final linear projection (optional but standard)\n",
    "proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "final_out = proj(multi_head_out)\n",
    "\n",
    "print(\"\\nMulti-Head Output shape:\", final_out.shape)\n",
    "print(\"\\nOutput for first 2 tokens:\\n\", final_out[:2])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
